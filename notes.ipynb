{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initial Step**\n",
    "using template.py we have created the structure of the project \n",
    "\n",
    "## **Why we use .gitkeep file ?**\n",
    "\n",
    "The .gitkeep file is a convention used to ensure that empty directories are tracked by Git. Git does not track empty directories by default, so placing a .gitkeep file inside an otherwise empty directory ensures that the directory is included in the repository.\n",
    "\n",
    "In your case, the .gitkeep file is likely used to ensure that the workflows directory is tracked by Git, even if there are no workflow files present yet. This can be useful for setting up the structure of your repository in anticipation of adding GitHub Actions workflows later.\n",
    "\n",
    "Here's a summary:\n",
    "\n",
    "Purpose: To ensure the workflows directory is tracked by Git.\n",
    "Usage: Place a .gitkeep file in an empty directory to include it in version control.\n",
    "You can remove the .gitkeep file once you add actual workflow files to the directory.\n",
    "\n",
    "## **Next Steps**\n",
    "\n",
    "then we are setting up the logging \n",
    "\n",
    "add the code in the init.py file of /components/logging\n",
    "\n",
    "after setting the custom logging functionality it is then connected to the main.py to get the logs\n",
    "\n",
    "then see there is a log folder created having file continouslog.log file where continous log is created\n",
    "\n",
    "now go inside the utils and write the code in common.py \n",
    "\n",
    "lets go to the reserach.ipynb to understand the importance of config box we have used in common.py \n",
    "\n",
    "here we understand why we have used config box and ensure_annotations\n",
    "we use config box because we want to access the dictionary values using dot operators which is not posible in dictionary \n",
    "\n",
    "ensure annotation is used for making sure that the datatype we have mentioned we get only that type of datatype only \n",
    "\n",
    "then we will be using samsum dataset and google --> pegasus-cnn_dailymail\n",
    "\n",
    "now writing codes in /research/textsummarizer.ipynb use google colab we will be requiring the gpus\n",
    "\n",
    "now create the file 1_data_ingestion.ipynb\n",
    "\n",
    "then add info into config.yaml\n",
    "\n",
    "every model like data ingestion will be taking some input and giving some output and this output we call as artifacts it can be folder , csv file , or we create the model\n",
    "\n",
    "adding dataclass in 1_data_ingestion.ipynb\n",
    "\n",
    "import the path of common utilits and constants\n",
    "\n",
    "adding constants to the src/textSummarizer/constants/__init__.py file \n",
    "\n",
    "after that creating configuration manager in 1_data_ingestion.ipynb\n",
    "\n",
    "configuration manager is importing the constant and creating artifact root directory\n",
    "\n",
    "then it creates the componets for extraction the dataset from github and unziping it creating the dataingestion\n",
    "\n",
    "now we will create config_entity add the dataclass of dataIngestion in it we created previously in src/textsummarizer/entity/__init__.py\n",
    "\n",
    "\n",
    "add the configuration manager class with importing of constants and utils/common.py file in /src/textsummarizer/config/configuration.py\n",
    "\n",
    "add dataingestion class in src/textsummarizer/components/data_ingestion.py\n",
    "\n",
    "now creating the src/textsummarizer/pipeline/stage_1_data_ingestion_pipeline.py and add the last block of code of 1_data_ingestion.ipynb file and import the necessary files\n",
    "\n",
    "now test the stage 1 of pipeline in main.py  and it runs well \n",
    "\n",
    "now we will be creating and adding code to the 2_data_transformation.ipynb by keeping the textsummarizer.ipynb as base \n",
    "\n",
    "update the code similarly and import the necessary files\n",
    "\n",
    "entitty/__init__.py (add the dataclass of datatransformation)\n",
    "\n",
    "config/configuartion.py(add get_data_transformation_config)\n",
    "\n",
    "create new file component/data_transformation.py (add DataTransformation Class)\n",
    "\n",
    "then create the new stage in pipeline\n",
    "create the file pipeline/stage_2_data_transformation.py file \n",
    "\n",
    "then test new stage in main.py\n",
    "\n",
    "now we will create the research/3_model_trainer.ipynb \n",
    "\n",
    "adding model parameters in the params.yaml file \n",
    "\n",
    "update the code similarly and import the necessary files\n",
    "\n",
    "entitty/__init__.py (add the dataclass of modeltrainerConfig)\n",
    "\n",
    "config/configuartion.py(add get_model_trainer_config)\n",
    "\n",
    "create new file component/model_trainer.py (add ModelTrainer Class)\n",
    "\n",
    "then create the new stage in pipeline\n",
    "create the file pipeline/stage_3_model_trainer.py file \n",
    "\n",
    "then test new stage in main.py\n",
    "\n",
    "now add the artifact paths in config.yaml file \n",
    "\n",
    "then create the research/4_data_evaluation.ipynb and add the content \n",
    "\n",
    "similarly stage 4 is done\n",
    "\n",
    "now create the pipeline/prediction_pipeline.py file add the thr prediction code to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
